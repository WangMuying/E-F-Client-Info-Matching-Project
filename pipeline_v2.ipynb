{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMH1ZEW8tJCwAL8G8zU2vwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WangMuying/E-F-Client-Info-Matching-Project/blob/main/pipeline_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0O6RtawpEQ2",
        "outputId": "e86ded30-4021-4abb-9d67-7f068c34090b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6xJTglepPk9",
        "outputId": "6e87a54d-7dce-4170-d0fd-3f5104bfdac3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_salesforce(file_path, encodings=['latin1', 'iso-8859-1', 'cp1252']):\n",
        "    # Try reading the CSV with a different encoding\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df_all = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"Successfully read the CSV file with {encoding} encoding.\")\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Failed to read the CSV file with {encoding} encoding.\")\n",
        "    else:\n",
        "        print(\"Unable to read the CSV file with the specified encodings.\")\n",
        "    return df_all\n",
        "\n",
        "\n",
        "def extract_name_from_email(email):\n",
        "  name_part = email.split('@')[0]\n",
        "  name_parts = re.split(r'[._]', name_part)\n",
        "  name = ' '.join(name_parts).title()\n",
        "  return name\n",
        "\n",
        "def identify_and_replace_email_leads(df, salesforce_name_key_list, email_column = 'Email'):\n",
        "  name_column = salesforce_name_key_list[0]\n",
        "  email_row_index = df[name_column].str.contains('@', na=False)\n",
        "  df['Name_from_email'] = df[email_row_index][email_column].apply(extract_name_from_email)\n",
        "  return df\n",
        "\n",
        "\n",
        "def make_composite_key(df_conference, df_all, salesforce_composite_key_list, salesforce_name_key_list):\n",
        "    attendees_df_llm = df_conference.copy() # attendees data\n",
        "    customers_df_llm = df_all.copy()  # salesforce data\n",
        "\n",
        "    # Fill na values with empty strings to make sure the Encoding model works\n",
        "    attendees_df_llm['First Name'] = attendees_df_llm['First Name'].fillna('').astype(str)\n",
        "    attendees_df_llm['Last Name'] = attendees_df_llm['Last Name'].fillna('').astype(str)\n",
        "    attendees_df_llm['Institution'] = attendees_df_llm['Institution'].fillna('').astype(str)\n",
        "\n",
        "    # if len(salesforce_name_key_list) == 1:\n",
        "    #   customers_df_llm = identify_and_replace_email_leads(customers_df_llm, salesforce_name_key_list, email_column = 'Email')\n",
        "\n",
        "    for key in salesforce_composite_key_list:\n",
        "        customers_df_llm[key] = customers_df_llm[key].fillna('').astype(str)\n",
        "\n",
        "    # Create Composite Keys\n",
        "    attendees_df_llm['Composite Key'] = attendees_df_llm['First Name'] + ' ' + attendees_df_llm['Last Name'] + ' ' + attendees_df_llm['Institution']\n",
        "    customers_df_llm['Composite Key'] = customers_df_llm.apply(lambda row: ' '.join([row[key] for key in salesforce_composite_key_list]), axis=1)\n",
        "    return (attendees_df_llm, customers_df_llm)\n",
        "\n",
        "\n",
        "# To process batch data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "\n",
        "# Function that compute embeddings for each batch in the data\n",
        "def compute_embeddings(model, device, text_list, batch_size=32):\n",
        "    dataset = TextDataset(text_list)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    embeddings = []\n",
        "    for batch in dataloader:\n",
        "        batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False, device=device)\n",
        "        embeddings.append(batch_embeddings)\n",
        "\n",
        "    return torch.cat(embeddings)\n",
        "\n",
        "# Function that pass keys to compute_embeddings() function\n",
        "def get_embeddings(model, device, attendees_df_llm, customers_df_llm, salesforce_name_key_list, salesforce_institution_key_list, batch_size=32):\n",
        "    # Get embeddings for the Composite Key, Name, Institution respectively\n",
        "    attendees_composite_embeddings = compute_embeddings(model, device, attendees_df_llm['Composite Key'].tolist(), batch_size)\n",
        "    customers_composite_embeddings = compute_embeddings(model, device, customers_df_llm['Composite Key'].tolist(), batch_size)\n",
        "\n",
        "    attendees_name_embeddings = compute_embeddings(model, device, (attendees_df_llm['First Name'] + ' ' + attendees_df_llm['Last Name']).tolist(), batch_size)\n",
        "    customers_name_embeddings = compute_embeddings(model, device, customers_df_llm.apply(lambda row: ' '.join([row[key] for key in salesforce_name_key_list]), axis=1).tolist(), batch_size)\n",
        "\n",
        "    attendees_institution_embeddings = compute_embeddings(model, device, attendees_df_llm['Institution'].tolist(), batch_size)\n",
        "    customers_account_embeddings = compute_embeddings(model, device, customers_df_llm.apply(lambda row: ' '.join([row[key] for key in salesforce_institution_key_list]), axis=1).tolist(), batch_size)\n",
        "\n",
        "    return (attendees_composite_embeddings, customers_composite_embeddings, attendees_name_embeddings, customers_name_embeddings, attendees_institution_embeddings, customers_account_embeddings)\n",
        "\n",
        "\n",
        "# Match\n",
        "def match_records(attendees_composite_embeddings, customers_composite_embeddings, attendees_name_embeddings, customers_name_embeddings, attendees_institution_embeddings, customers_account_embeddings, attendees_df_llm, customers_df_llm, lower_bound_threshold=0.5, threshold=0.90):\n",
        "    results = []\n",
        "    for i, attendee_embedding in enumerate(attendees_composite_embeddings):\n",
        "\n",
        "        # Calculate the cosine similarity scores for each composite key\n",
        "        composite_scores = util.pytorch_cos_sim(attendee_embedding, customers_composite_embeddings)\n",
        "        max_composite_score, max_composite_idx = torch.max(composite_scores, dim=1)\n",
        "\n",
        "        # Caclulate the cosine similarity scores for the name\n",
        "        name_scores = util.pytorch_cos_sim(attendees_name_embeddings[i], customers_name_embeddings)\n",
        "        max_name_score = name_scores[0, max_composite_idx].item()\n",
        "\n",
        "        # Calculate the cosine similarity scores for the institution\n",
        "        institution_scores = util.pytorch_cos_sim(attendees_institution_embeddings[i], customers_account_embeddings)\n",
        "        max_institution_score = institution_scores[0, max_composite_idx].item()\n",
        "\n",
        "        # If ANY of the scores are below the lower bound threshold, then it is considered an unmatched record\n",
        "        if max_composite_score.item() < lower_bound_threshold or max_name_score < lower_bound_threshold or max_institution_score < lower_bound_threshold:\n",
        "            results.append((attendees_df_llm.iloc[i]['Composite Key'], None, max_composite_score.item(), max_name_score, max_institution_score, \"Unmatched\"))\n",
        "        # If ALL of the scores are above the threshold, then it is considered a matched record\n",
        "        elif max_composite_score.item() >= threshold and max_name_score >= threshold and max_institution_score >= threshold:\n",
        "            best_match = customers_df_llm.iloc[max_composite_idx.item()]['Composite Key']\n",
        "            results.append((attendees_df_llm.iloc[i]['Composite Key'], best_match, max_composite_score.item(), max_name_score, max_institution_score, \"Matched\"))\n",
        "        # If ANY of the scores are below the review threshold, then it is considered a review record\n",
        "        else:\n",
        "            best_match = customers_df_llm.iloc[max_composite_idx.item()]['Composite Key']\n",
        "            results.append((attendees_df_llm.iloc[i]['Composite Key'], best_match, max_composite_score.item(), max_name_score, max_institution_score, \"Review\"))\n",
        "\n",
        "    # Create a DataFrame from the results\n",
        "    results_df = pd.DataFrame(results, columns=['Attendee Composite Key', 'Matched Customer Composite Key', 'Composite Similarity Score', 'Name Similarity Score', 'Institution Similarity Score', 'Status'])\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def update_id(row, salesforce_id):\n",
        "    # if (row['Status'] == 'Matched') | (row['Review'] == 1) : # matched & unmatched (NaN) or reviewed to be matched (1)\n",
        "    if row['Review'] != 0:\n",
        "        return row[salesforce_id + ' Customer']\n",
        "    return"
      ],
      "metadata": {
        "id": "eG0FptF9pWTb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  # Load Model\n",
        "  model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_matching_output(model, device, salesforce_client_type_list, salesforce_feature_dict_list, ind):\n",
        "  salesforce_client_type = salesforce_client_type_list[ind]\n",
        "  salesforce_feature_dict = salesforce_feature_dict_list[ind]\n",
        "\n",
        "  print('-'*100)\n",
        "  print(f'Process starts! Matching E&F data with Salesforce all_{salesforce_client_type} data...')\n",
        "  print('-'*100)\n",
        "\n",
        "  # Read data\n",
        "  salesforce_data_path = f'2024_06_26_salesforce_all_{salesforce_client_type}.csv'\n",
        "  df_salesforce = read_salesforce(salesforce_data_path)\n",
        "\n",
        "  if ind == 0:\n",
        "    enf_data_path = '2024_06_26_e&f_june_conference_attendee_full_list.csv'\n",
        "  else:\n",
        "    enf_data_path = f'e&f_unmatched_with_{salesforce_client_type_list[ind-1]}.csv'\n",
        "\n",
        "  unmatched_enf = pd.read_csv(enf_data_path)\n",
        "\n",
        "  all_composite_key_list = salesforce_feature_dict.get('salesforce_composite_key_list')\n",
        "  all_name_key_list = salesforce_feature_dict['salesforce_name_key_list']\n",
        "  all_institution_key_list = salesforce_feature_dict['salesforce_institution_key_list']\n",
        "  salesforce_id = salesforce_feature_dict['salesforce_id']\n",
        "\n",
        "  # Make composite keys\n",
        "  attendees_df_llm, customers_df_llm = make_composite_key(unmatched_enf, df_salesforce, all_composite_key_list, all_name_key_list)\n",
        "\n",
        "\n",
        "  # Get embeddings for the composite keys, names, and institutions respectively\n",
        "  # This should take approx 10-15 mins if gpu else 30+ mins.\n",
        "  attendees_composite_embeddings, customers_composite_embeddings, attendees_name_embeddings, customers_name_embeddings, attendees_institution_embeddings, customers_account_embeddings = get_embeddings(model, device, attendees_df_llm, customers_df_llm, all_name_key_list, all_institution_key_list)\n",
        "\n",
        "  # Match records\n",
        "  results_df = match_records(attendees_composite_embeddings, customers_composite_embeddings, attendees_name_embeddings, customers_name_embeddings, attendees_institution_embeddings, customers_account_embeddings, attendees_df_llm, customers_df_llm, lower_bound_threshold=0.5, threshold=0.90)\n",
        "  assert len(results_df) == len(attendees_df_llm), \"Length of results_df doesn't match length of attendees_df_llm. There may be duplicates in each data frame.\"\n",
        "  results_df.drop_duplicates(subset=['Attendee Composite Key'], keep='first', inplace=True)\n",
        "  attendees_df_llm.drop_duplicates(subset=['Composite Key'], keep='first', inplace=True)\n",
        "  assert len(results_df) == len(attendees_df_llm), \"Lengths still unmatched after dropping duplicates.\"\n",
        "\n",
        "\n",
        "  ## Step 1. Merge the results with the attendees data\n",
        "  output = results_df.merge(attendees_df_llm, how=\"left\", left_on='Attendee Composite Key', right_on='Composite Key')\n",
        "  # Drop duplicates\n",
        "  if output.duplicated(subset=['Attendee Composite Key']).sum() > 0:\n",
        "      output.drop_duplicates(subset=['Attendee Composite Key'], keep='first', inplace=True)\n",
        "  assert len(results_df) == len(output), \"Merging back E&F (attendees) data goes wrong.\"\n",
        "  output = output.drop(columns=['Composite Key'])\n",
        "  ## Delete Salesforce column later for other conference data. Here, Salesforce is just a (assume true) label for accuracy checking\n",
        "  # output = output.rename(columns={'First Name': 'First Name Attendee', 'Last Name': 'Last Name Attendee', 'Institution': 'Institution Attendee', 'Salesforce': 'Salesforce Attendee'})\n",
        "  output = output.rename(columns={'First Name': 'First Name Attendee', 'Last Name': 'Last Name Attendee', 'Institution': 'Institution Attendee'})\n",
        "  len_after_merge_1 = len(output)\n",
        "\n",
        "  ## Step 2. Merge the results with the customers data\n",
        "  output = output.merge(customers_df_llm, how=\"left\", left_on='Matched Customer Composite Key', right_on='Composite Key') # Contact ID comes from this table\n",
        "  # Drop duplicates\n",
        "  if output.duplicated(subset=['Attendee Composite Key']).sum() > 0:\n",
        "      output.drop_duplicates(subset=['Attendee Composite Key'], keep='first', inplace=True)\n",
        "  assert len_after_merge_1 == len(output), \"Merging back Salesforce data goes wrong.\"\n",
        "\n",
        "  # Drop all the columns in customers_df_llm except for salesforce_id\n",
        "  output = output.drop(columns=['Composite Key'])\n",
        "  output = output.drop(columns=all_composite_key_list)\n",
        "  # output = output.rename(columns={'Contact ID': 'Contact ID Customer'})\n",
        "  output = output.rename(columns={salesforce_id: salesforce_id + ' Customer'})\n",
        "\n",
        "  cols_to_keep = results_df.columns.tolist()\n",
        "  cols_to_keep.extend(['First Name Attendee',\t'Last Name Attendee',\t'Institution Attendee', salesforce_id + ' Customer'])\n",
        "  output = output[cols_to_keep]\n",
        "  output.to_excel(f'output_all_{salesforce_client_type}.xlsx', index=False)\n",
        "\n",
        "\n",
        "  while True:\n",
        "    print('')\n",
        "    print(f'''\n",
        "    ----------------------------------------------------------------------------\n",
        "    output_all_{salesforce_client_type}.xlsx is saved.\n",
        "    ----------------------------------------------------------------------------\n",
        "    1. Open `output_all_{salesforce_client_type}.xlsx`.\n",
        "    2. Add a new column called `Review`.\n",
        "    3. For entries with `Status` being `Review`, fill 1 if reviewed to be matched, 0 if unmatched.\n",
        "    4. Save the file as `output_all_{salesforce_client_type}_after_review.xlsx`.\n",
        "    ----------------------------------------------------------------------------\n",
        "    ''' )\n",
        "\n",
        "    status = input(\"Review done? (y/n): \")\n",
        "    if status.lower() == 'y':\n",
        "      break\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "\n",
        "  output_after_review = pd.read_excel(f'output_all_{salesforce_client_type}_after_review.xlsx')\n",
        "  output_after_review[salesforce_id + ' Customer'] = output_after_review.apply(update_id, salesforce_id=salesforce_id, axis=1)\n",
        "  rest_of_enf = output_after_review[output_after_review[salesforce_id + ' Customer'].isnull()][['First Name Attendee',\t'Last Name Attendee',\t'Institution Attendee']]\n",
        "  rest_of_enf.rename(columns={'First Name Attendee': \"First Name\", 'Last Name Attendee': \"Last Name\", 'Institution Attendee': \"Institution\"}, inplace=True)\n",
        "  rest_of_enf.to_csv(f'e&f_unmatched_with_{salesforce_client_type}.csv', index=False)\n",
        "  print('----------------------------------------------------------------------------')\n",
        "  print(f'e&f_unmatched_with_{salesforce_client_type}.csv is saved.')\n",
        "\n",
        "  ready_for_upload = output_after_review[[salesforce_id + ' Customer']].rename(columns={salesforce_id + ' Customer': salesforce_id})\n",
        "  # ready_for_upload = ready_for_upload.rename(columns={'First Name Attendee': \"First Name\", 'Last Name Attendee': \"Last Name\", 'Institution Attendee': \"Institution\",'Contact ID Customer': 'Contact ID'})\n",
        "  ready_for_upload = ready_for_upload[~ready_for_upload[salesforce_id].isnull()]\n",
        "  ready_for_upload.to_excel(f'ready_for_upload_all_{salesforce_client_type}.xlsx', index=False)\n",
        "\n",
        "  print(f'''\n",
        "  ----------------------------------------------------------------------------\n",
        "  ready_for_upload_all_{salesforce_client_type}.xlsx is saved.\n",
        "  ----------------------------------------------------------------------------\n",
        "  Matching finished.\n",
        "  ''')\n",
        "  return"
      ],
      "metadata": {
        "id": "8VSYnfN4L4hM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzgUIVu152VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enf_data_path = '2024_06_26_e&f_june_conference_attendee_full_list.csv'\n",
        "salesforce_client_type_list = ['contacts', 'leads', 'unidentified_leads']\n",
        "salesforce_feature_dict_list = [{'salesforce_client_type': 'contacts',\n",
        "  'salesforce_composite_key_list': ['First Name', 'Last Name', 'Account Name'],\n",
        "  'salesforce_name_key_list': ['First Name', 'Last Name'],\n",
        "  'salesforce_institution_key_list': ['Account Name'],\n",
        "  'salesforce_id': 'Contact ID'\n",
        "  },\n",
        "\n",
        " {'salesforce_client_type': 'leads',\n",
        "  'salesforce_composite_key_list': ['First Name', 'Last Name', 'Account'],\n",
        "  'salesforce_name_key_list': ['First Name', 'Last Name'],\n",
        "  'salesforce_institution_key_list': ['Account'],\n",
        "  'salesforce_id': 'Lead ID'},\n",
        "\n",
        " {'salesforce_client_type': 'unidentified_leads',\n",
        "  'salesforce_composite_key_list': ['Unidentified Lead: Unidentified Leads', 'Firm Name'],\n",
        "  'salesforce_name_key_list': ['Unidentified Lead: Unidentified Leads'],\n",
        "  'salesforce_institution_key_list': ['Firm Name'],\n",
        "  'salesforce_id': 'Unidentified Lead: ID'}\n",
        "]"
      ],
      "metadata": {
        "id": "74epNQ2-5R2y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check availability for gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('-'*100)\n",
        "print(\"Device using is\", device)\n",
        "print('-'*100)\n",
        "\n",
        "# Get pre-trained model\n",
        "model = get_model()\n",
        "model.to(device)\n",
        "\n",
        "get_matching_output(model, device, salesforce_client_type_list, salesforce_feature_dict_list, ind = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfNOjnx85i4H",
        "outputId": "8a0ac94d-734d-4937-c604-60d87659b6e8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Device using is cuda\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Process starts! Matching E&F data with Salesforce all_unidentified_leads data...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Successfully read the CSV file with latin1 encoding.\n",
            "\n",
            "\n",
            "    ----------------------------------------------------------------------------\n",
            "    output_all_unidentified_leads.xlsx is saved. \n",
            "    ----------------------------------------------------------------------------\n",
            "    1. Open `output_all_unidentified_leads.xlsx`.\n",
            "    2. Add a new column called `Review`.\n",
            "    3. For entries with `Status` being `Review`, fill 1 if reviewed to be matched, 0 if unmatched.\n",
            "    4. Save the file as `output_all_unidentified_leads_after_review.xlsx`.\n",
            "    ----------------------------------------------------------------------------\n",
            "    \n",
            "Review done? (y/n): y\n",
            "----------------------------------------------------------------------------\n",
            "e&f_unmatched_with_unidentified_leads.csv is saved.\n",
            "\n",
            "  ----------------------------------------------------------------------------\n",
            "  ready_for_upload_all_unidentified_leads.xlsx is saved.\n",
            "  ----------------------------------------------------------------------------\n",
            "  Matching finished.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tT_PVYE35UoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZHRNzrq5Ulr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouVltdBx5Ugd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOpyw6n05Ud5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}